{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6680d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years we want to scrape MVP data for\n",
    "# Range function is non inclusive of last year\n",
    "\n",
    "years = list(range(1991,2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b086dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In url, the {} replace the year, so we can iterate through the years\n",
    "\n",
    "url_start = \"https://www.basketball-reference.com/awards/awards_{}.html#mvp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for loop to download the html page for every year in our list\n",
    "# again {} will create a new file named for every year in our folder\n",
    "\n",
    "for year in years:\n",
    "    url = url_start.format(year)\n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs/{}.html\".format(year),\"w+\", encoding='UTF8') as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the tables we need\n",
    "\n",
    "with open(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\1991.html\", encoding = 'UTF8') as f:\n",
    "    page = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page, 'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b86617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an overheader row in our table we need to remove so it will import into pandas smoothly\n",
    "#  'Voting', 'PerGame', 'Shooting', and 'Advanced'\n",
    "\n",
    "soup.find('tr', class_ = \"over_header\").decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfce024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate our mvp table and pull it out\n",
    "\n",
    "mvp_table = soup.find(id = \"mvp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8100aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read mvp_table into pandas and store it as dataframe\n",
    "\n",
    "mvp_1991 = pd.read_html(str(mvp_table))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up the process we used for one year using a for loop to get all the years\n",
    "# append each year to the same dataframe so we end up with one dataframe containing \n",
    "# all MVP votes from 1991-2023\n",
    "\n",
    "dfs = []\n",
    "for year in years:\n",
    "    with open(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\{}.html\".format(year), encoding = 'UTF8') as f:\n",
    "        page = f.read()\n",
    "    soup = BeautifulSoup(page, 'html')\n",
    "    soup.find('tr', class_ = \"over_header\").decompose()\n",
    "    mvp_table = soup.find(id = \"mvp\")\n",
    "    mvp = pd.read_html(str(mvp_table))[0]\n",
    "    mvp[\"Year\"] = year        # add a year column, so when dfs are combined we can still differentiate seasons\n",
    "    \n",
    "    dfs.append(mvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2936a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1496780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect all our lists of dfs into one single dataframe\n",
    "\n",
    "mvps = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db923d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74248ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this dataframe as a csv file so we can look at it later\n",
    "\n",
    "mvps.to_csv(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\mvps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c62c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps = pd.read_csv(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\mvps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0727d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mvps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need team stats because Wins and Losses heavily influence MVP voting\n",
    "\n",
    "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc98d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for loop to download the html page for every year in our list, this time for team stats/standings\n",
    "\n",
    "for year in years:\n",
    "    url = team_stats_url.format(year)\n",
    "\n",
    "    data = requests.get(url)\n",
    "\n",
    "    with open(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\teams/{}.html\".format(year), \"w+\", encoding = \"UTF8\") as f:\n",
    "        f.write(data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222a14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we did with the MVP voting, we are going to pull the tables of interest from the html files\n",
    "# we are using the division standings table due to how it is stored in the html page\n",
    "\n",
    "\n",
    "dfs = []\n",
    "for year in years:\n",
    "    with open(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\teams/{}.html\".format(year), encoding = \"UTF8\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, 'html')\n",
    "    tHeads = soup.find_all('tr', class_ = 'thead')      # find all thead items, then loop through and delete each one\n",
    "    for tHead in tHeads:                                # this will remove all the thead items in our table\n",
    "        tHead.decompose()                               # which include all the division labels\n",
    "    team_table = soup.find(id = \"divs_standings_E\")\n",
    "    team = pd.read_html(str(team_table))[0]\n",
    "    team[\"Year\"] = year\n",
    "    team[\"Team\"] = team[\"Eastern Conference\"]           # reassign the conference column to team name\n",
    "    del team[\"Eastern Conference\"]                      # delete the conference column since we don't need it\n",
    "    dfs.append(team)\n",
    "    \n",
    "    soup = BeautifulSoup(page, 'html')\n",
    "    tHeads = soup.find_all('tr', class_ = 'thead')\n",
    "    for tHead in tHeads:\n",
    "        tHead.decompose()\n",
    "    team_table = soup.find(id = \"divs_standings_W\")\n",
    "    team = pd.read_html(str(team_table))[0]\n",
    "    team[\"Year\"] = year\n",
    "    team[\"Team\"] = team[\"Western Conference\"]\n",
    "    del team[\"Western Conference\"]\n",
    "    dfs.append(team)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our file containing all team stats from 1991-2023 to a csv file\n",
    "\n",
    "teams.to_csv(r\"C:\\Users\\wrsno\\Documents\\NBA_MVPs\\teams\\teams.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
